{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_8gWZMV3_YO",
        "outputId": "f66a054e-e7ec-459d-dab1-d0046a5b5ad6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!pip3 install face_recognition\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import face_recognition\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import os\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_classes,latent_dim= 2048, lstm_layers=1 , hidden_dim = 2048, bidirectional = False):\n",
        "        super(Model, self).__init__()\n",
        "        model = models.resnext50_32x4d(pretrained = True)\n",
        "        self.model = nn.Sequential(*list(model.children())[:-2])\n",
        "        self.lstm = nn.LSTM(latent_dim,hidden_dim, lstm_layers,  bidirectional)\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.dp = nn.Dropout(0.4)\n",
        "        self.linear1 = nn.Linear(2048,num_classes)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "    def forward(self, x):\n",
        "        batch_size,seq_length, c, h, w = x.shape\n",
        "        x = x.view(batch_size * seq_length, c, h, w)\n",
        "        fmap = self.model(x)\n",
        "        x = self.avgpool(fmap)\n",
        "        x = x.view(batch_size,seq_length,2048)\n",
        "        x_lstm,_ = self.lstm(x,None)\n",
        "        return fmap,self.dp(self.linear1(x_lstm[:,-1,:]))\n",
        "\n",
        "\n",
        "im_size = 112\n",
        "mean=[0.485, 0.456, 0.406]\n",
        "std=[0.229, 0.224, 0.225]\n",
        "sm = nn.Softmax()\n",
        "inv_normalize =  transforms.Normalize(mean=-1*np.divide(mean,std),std=np.divide([1,1,1],std))\n",
        "def im_convert(tensor):\n",
        "    \"\"\" Display a tensor as an image. \"\"\"\n",
        "    image = tensor.to(\"cpu\").clone().detach()\n",
        "    image = image.squeeze()\n",
        "    image = inv_normalize(image)\n",
        "    image = image.numpy()\n",
        "    image = image.transpose(1,2,0)\n",
        "    image = image.clip(0, 1)\n",
        "    cv2.imwrite('./2.png',image*255)\n",
        "    return image\n",
        "\n",
        "def predict(model,img,path = './'):\n",
        "  fmap,logits = model(img.to('cuda'))\n",
        "  params = list(model.parameters())\n",
        "  weight_softmax = model.linear1.weight.detach().cpu().numpy()\n",
        "  logits = sm(logits)\n",
        "  _,prediction = torch.max(logits,1)\n",
        "  confidence = logits[:,int(prediction.item())].item()*100\n",
        "  print('confidence of prediction:',logits[:,int(prediction.item())].item()*100)\n",
        "  idx = np.argmax(logits.detach().cpu().numpy())\n",
        "  bz, nc, h, w = fmap.shape\n",
        "  out = np.dot(fmap[-1].detach().cpu().numpy().reshape((nc, h*w)).T,weight_softmax[idx,:].T)\n",
        "  predict = out.reshape(h,w)\n",
        "  predict = predict - np.min(predict)\n",
        "  predict_img = predict / np.max(predict)\n",
        "  predict_img = np.uint8(255*predict_img)\n",
        "  out = cv2.resize(predict_img, (im_size,im_size))\n",
        "  heatmap = cv2.applyColorMap(out, cv2.COLORMAP_JET)\n",
        "  img = im_convert(img[:,-1,:,:,:])\n",
        "  result = heatmap * 0.5 + img*0.8*255\n",
        "  cv2.imwrite('/content/1.png',result)\n",
        "  result1 = heatmap * 0.5/255 + img*0.8\n",
        "  r,g,b = cv2.split(result1)\n",
        "  result1 = cv2.merge((r,g,b))\n",
        "  plt.imshow(result1)\n",
        "  plt.show()\n",
        "  return [int(prediction.item()),confidence]\n",
        "#img = train_data[100][0].unsqueeze(0)\n",
        "#predict(model,img)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import face_recognition\n",
        "class validation_dataset(Dataset):\n",
        "    def __init__(self,video_names,sequence_length = 60,transform = None):\n",
        "        self.video_names = video_names\n",
        "        self.transform = transform\n",
        "        self.count = sequence_length\n",
        "    def __len__(self):\n",
        "        return len(self.video_names)\n",
        "    def __getitem__(self,idx):\n",
        "        video_path = self.video_names[idx]\n",
        "        frames = []\n",
        "        a = int(100/self.count)\n",
        "        first_frame = np.random.randint(0,a)\n",
        "        for i,frame in enumerate(self.frame_extract(video_path)):\n",
        "            #if(i % a == first_frame):\n",
        "            faces = face_recognition.face_locations(frame)\n",
        "            try:\n",
        "              top,right,bottom,left = faces[0]\n",
        "              frame = frame[top:bottom,left:right,:]\n",
        "            except:\n",
        "              pass\n",
        "            frames.append(self.transform(frame))\n",
        "            if(len(frames) == self.count):\n",
        "              break\n",
        "        #print(\"no of frames\",len(frames))\n",
        "        frames = torch.stack(frames)\n",
        "        frames = frames[:self.count]\n",
        "        return frames.unsqueeze(0)\n",
        "    def frame_extract(self,path):\n",
        "      vidObj = cv2.VideoCapture(path)\n",
        "      success = 1\n",
        "      while success:\n",
        "          success, image = vidObj.read()\n",
        "          if success:\n",
        "              yield image\n",
        "def im_plot(tensor):\n",
        "    image = tensor.cpu().numpy().transpose(1,2,0)\n",
        "    b,g,r = cv2.split(image)\n",
        "    image = cv2.merge((r,g,b))\n",
        "    image = image*[0.22803, 0.22145, 0.216989] +  [0.43216, 0.394666, 0.37645]\n",
        "    image = image*255.0\n",
        "    plt.imshow(image.astype(int))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#Code for making prediction\n",
        "im_size = 112\n",
        "mean=[0.485, 0.456, 0.406]\n",
        "std=[0.229, 0.224, 0.225]\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "                                        transforms.ToPILImage(),\n",
        "                                        transforms.Resize((im_size,im_size)),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean,std)])\n",
        "path_to_videos = ['/content/gdrive/MyDrive/Colab_Notebooks/Celeb-DF/YouTube-real/*.mp4',\n",
        "                                   '/content/gdrive/MyDrive/Colab_Notebooks/Celeb-DF/Celeb-real/*.mp4']\n",
        "\n",
        "path_to_videos = ['/content/gdrive/MyDrive/Colab_Notebooks/Celeb-DF/YouTube-real/*.mp4',\n",
        "                                   '/content/gdrive/MyDrive/Colab_Notebooks/Celeb-DF/Celeb-real/*.mp4']\n",
        "\n",
        "\n",
        "path_to_videos= [\"/content/gdrive/MyDrive/Colab_Notebooks/Celeb-DF/Celeb-real/*.mp4\"]\n",
        "\n",
        "video_dataset = validation_dataset(path_to_videos,sequence_length = 10,transform = train_transforms)\n",
        "model = Model(2).cuda()\n",
        "path_to_model = '/content/gdrive/MyDrive/checkpoint.pt'\n",
        "model.load_state_dict(torch.load(path_to_model))\n",
        "model.eval()\n",
        "for i in range(0,len(path_to_videos)):\n",
        "  print(path_to_videos[i])\n",
        "  prediction = predict(model,video_dataset[i],'./')\n",
        "  if prediction[0] == 1:\n",
        "    print(\"REAL\")\n",
        "  else:\n",
        "    print(\"FAKE\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "7-YqnJpW4YbA",
        "outputId": "559533b3-fcbb-4558-cccd-bc3f4d77f2fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: face_recognition in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
            "Requirement already satisfied: face-recognition-models>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (0.3.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (8.1.7)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (19.24.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_recognition) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from face_recognition) (9.4.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error while calling cudaGetDevice(&the_device_id) in file /tmp/pip-install-2otkmeu8/dlib_d60ddf776fdd4b7fa1d7052efcfe46ba/dlib/cuda/gpu_data.cpp:204. code: 35, reason: CUDA driver version is insufficient for CUDA runtime version",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e9cf1d324eee>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/face_recognition/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1.2.3'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_image_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_face_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_landmarks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_encodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompare_faces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/face_recognition/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mcnn_face_detection_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_face_detector_model_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mcnn_face_detector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_face_detection_model_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_face_detection_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mface_recognition_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_recognition_model_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error while calling cudaGetDevice(&the_device_id) in file /tmp/pip-install-2otkmeu8/dlib_d60ddf776fdd4b7fa1d7052efcfe46ba/dlib/cuda/gpu_data.cpp:204. code: 35, reason: CUDA driver version is insufficient for CUDA runtime version"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#optional : If you want to pass full frame for prediction instead of face cropped frame\n",
        "#code for full frame processing\n",
        "class validation_dataset(Dataset):\n",
        "    def __init__(self,video_names,sequence_length = 60,transform = None):\n",
        "        self.video_names = video_names\n",
        "        self.transform = transform\n",
        "        self.count = sequence_length\n",
        "    def __len__(self):\n",
        "        return len(self.video_names)\n",
        "    def __getitem__(self,idx):\n",
        "        video_path = self.video_names[idx]\n",
        "        frames = []\n",
        "        a = int(100/self.count)\n",
        "        first_frame = np.random.randint(0,a)\n",
        "        for i,frame in enumerate(self.frame_extract(video_path)):\n",
        "          frames.append(self.transform(frame))\n",
        "          if(len(frames) == self.count):\n",
        "            break\n",
        "        frames = torch.stack(frames)\n",
        "        frames = frames[:self.count]\n",
        "        return frames.unsqueeze(0)\n",
        "    def frame_extract(self,path):\n",
        "      vidObj = cv2.VideoCapture(path)\n",
        "      success = 1\n",
        "      while success:\n",
        "          success, image = vidObj.read()\n",
        "          if success:\n",
        "              yield image"
      ],
      "metadata": {
        "id": "JYs4qN_1LkgR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}