{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alapa\\AppData\\Local\\Temp\\ipykernel_24520\\2459639873.py:21: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import seaborn as sn\n",
    "import random\n",
    "import copy\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import face_recognition\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torchvision.models import ResNeXt50_32X4D_Weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the file is corrupted or not\n",
    "def validate_video(vid_path,train_transforms):\n",
    "      transform = train_transforms\n",
    "      count = 20\n",
    "      video_path = vid_path\n",
    "      frames = []\n",
    "      a = int(100/count)\n",
    "      first_frame = np.random.randint(0,a)\n",
    "      temp_video = video_path.split('\\\\')[-1]\n",
    "      for i,frame in enumerate(frame_extract(video_path)):\n",
    "        frames.append(transform(frame))\n",
    "        if(len(frames) == count):\n",
    "          break\n",
    "      frames = torch.stack(frames)\n",
    "      frames = frames[:count]\n",
    "      return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_extract(path):\n",
    "  vidObj = cv2.VideoCapture(path) \n",
    "  success = 1\n",
    "  while success:\n",
    "      success, image = vidObj.read()\n",
    "      if success:\n",
    "          yield image\n",
    "\n",
    "im_size = 112\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_transforms = transforms.Compose([\n",
    "  transforms.ToPILImage(),\n",
    "  transforms.Resize((im_size,im_size)),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(mean,std)])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "  transforms.ToPILImage(),\n",
    "  transforms.Resize((im_size,im_size)),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(mean,std)])\n",
    "video_fil =  glob.glob('C:/Users/alapa/Documents/Deepfake_detection_using_deep_learning-master/Deepfake_detection_using_deep_learning-master/dfdc/dfdc_train_part_12/*.mp4')\n",
    "#video_fil += glob.glob('C:/Users/alapa/Documents/Deepfake_detection_using_deep_learning-master/Deepfake_detection_using_deep_learning-master/Celeb-DF/YouTube-real/*.mp4')\n",
    "video_fil += glob.glob('C:/Users/alapa/Documents/Deepfake_detection_using_deep_learning-master/Deepfake_detection_using_deep_learning-master/dfdc/dfdc_train_part_11/*.mp4')\n",
    "\n",
    "print(\"Total no of videos :\" , len(video_fil))\n",
    "print(video_fil)\n",
    "count = 0;\n",
    "for i in video_fil:\n",
    "  try:\n",
    "    count+=1\n",
    "    validate_video(i,train_transforms)\n",
    "  except:\n",
    "    print(\"Number of video processed: \" , count ,\" Remaining : \" , (len(video_fil) - count))\n",
    "    print(\"Corrupted video is : \" , i)\n",
    "    continue\n",
    "print((len(video_fil) - count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_files =  glob.glob('C:/Users/alapa/Documents/Deepfake_detection_using_deep_learning-master/Deepfake_detection_using_deep_learning-master/dfdc/dfdc_train_part_12/*.mp4')\n",
    "#video_files += glob.glob('C:/Users/alapa/Documents/Deepfake_detection_using_deep_learning-master/Deepfake_detection_using_deep_learning-master/Celeb-DF/YouTube-real/*.mp4')\n",
    "video_files += glob.glob('C:/Users/alapa/Documents/Deepfake_detection_using_deep_learning-master/Deepfake_detection_using_deep_learning-master/dfdc/dfdc_train_part_11/*.mp4')\n",
    "\n",
    "random.shuffle(video_files)\n",
    "random.shuffle(video_files)\n",
    "frame_count = []\n",
    "for video_file in video_files:\n",
    "  cap = cv2.VideoCapture(video_file)\n",
    "  if(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))<100):\n",
    "    video_files.remove(video_file)\n",
    "    continue\n",
    "  frame_count.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "print(\"frames are \" , frame_count)\n",
    "print(\"Total no of video: \" , len(frame_count))\n",
    "print('Average frame per video:',np.mean(frame_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the video name and labels from csv\n",
    "class video_dataset(Dataset):\n",
    "    def __init__(self,video_names,labels,sequence_length = 60,transform = None):\n",
    "        self.video_names = video_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.count = sequence_length\n",
    "    def __len__(self):\n",
    "        return len(self.video_names)\n",
    "    def __getitem__(self,idx):\n",
    "        video_path = self.video_names[idx]\n",
    "        frames = []\n",
    "        a = int(100/self.count)\n",
    "        first_frame = np.random.randint(0,a)\n",
    "        temp_video = video_path.split('\\\\')[-1]\n",
    "        #print(temp_video)\n",
    "        label = self.labels.iloc[(labels.loc[labels[\"file\"] == temp_video].index.values[0]),1]\n",
    "        if(label == 'FAKE'):\n",
    "          label = 0\n",
    "        if(label == 'REAL'):\n",
    "          label = 1\n",
    "        for i,frame in enumerate(self.frame_extract(video_path)):\n",
    "          frames.append(self.transform(frame))\n",
    "          if(len(frames) == self.count):\n",
    "            break\n",
    "        frames = torch.stack(frames)\n",
    "        frames = frames[:self.count]\n",
    "        #print(\"length:\" , len(frames), \"label\",label)\n",
    "        return frames,label\n",
    "    def frame_extract(self,path):\n",
    "      vidObj = cv2.VideoCapture(path) \n",
    "      success = 1\n",
    "      while success:\n",
    "          success, image = vidObj.read()\n",
    "          if success:\n",
    "              yield image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the image\n",
    "def im_plot(tensor):\n",
    "    image = tensor.cpu().numpy().transpose(1,2,0)\n",
    "    b,g,r = cv2.split(image)\n",
    "    image = cv2.merge((r,g,b))\n",
    "    image = image*[0.22803, 0.22145, 0.216989] +  [0.43216, 0.394666, 0.37645]\n",
    "    image = image*255.0\n",
    "    plt.imshow(image.astype(int))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of fake and real videos\n",
    "def number_of_real_and_fake_videos(data_list):\n",
    "  header_list = [\"file\",\"label\"]\n",
    "  lab = pd.read_csv('C:/Users/alapa/Documents/Deepfake_detection_using_deep_learning-master/Deepfake_detection_using_deep_learning-master/abhijit/Model Creation/labels/dfdc/Global_metadata.csv',names=header_list)\n",
    "  fake = 0\n",
    "  real = 0\n",
    "  for i in data_list:\n",
    "    temp_video = i.split('\\\\')[-1]\n",
    "    label = lab.iloc[(labels.loc[labels[\"file\"] == temp_video].index.values[0]),1]\n",
    "    if(label == 'FAKE'):\n",
    "      fake+=1\n",
    "    if(label == 'REAL'):\n",
    "      real+=1\n",
    "  return real,fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the labels and video in data loader\n",
    "header_list = [\"file\",\"label\"]\n",
    "labels = pd.read_csv('C:/Users/alapa/Documents/Deepfake_detection_using_deep_learning-master/Deepfake_detection_using_deep_learning-master/abhijit/Model Creation/labels/dfdc/Global_metadata.csv',names=header_list)\n",
    "#print(labels)\n",
    "train_videos = video_files[:int(0.8*len(video_files))]\n",
    "valid_videos = video_files[int(0.8*len(video_files)):]\n",
    "print(\"train : \" , len(train_videos))\n",
    "print(\"test : \" , len(valid_videos))\n",
    "# train_videos,valid_videos = train_test_split(data,test_size = 0.2)\n",
    "# print(train_videos)\n",
    "\n",
    "print(\"TRAIN: \", \"Real:\",number_of_real_and_fake_videos(train_videos)[0],\" Fake:\",number_of_real_and_fake_videos(train_videos)[1])\n",
    "print(\"TEST: \", \"Real:\",number_of_real_and_fake_videos(valid_videos)[0],\" Fake:\",number_of_real_and_fake_videos(valid_videos)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = video_dataset(train_videos,labels,sequence_length = 10,transform = train_transforms)\n",
    "#print(train_data)\n",
    "val_data = video_dataset(valid_videos,labels,sequence_length = 10,transform = train_transforms)\n",
    "train_loader = DataLoader(train_data,batch_size = 4,shuffle = True,num_workers = 0)\n",
    "valid_loader = DataLoader(val_data,batch_size = 4,shuffle = True,num_workers = 0)\n",
    "image,label = train_data[0]\n",
    "im_plot(image[0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with feature visualization\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes,latent_dim= 2048, lstm_layers=1 , hidden_dim = 2048, bidirectional = False):\n",
    "        super(Model, self).__init__()\n",
    "        model = models.resnext50_32x4d(weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1) #Residual Network CNN\n",
    "        self.model = nn.Sequential(*list(model.children())[:-2])\n",
    "        self.lstm = nn.LSTM(latent_dim,hidden_dim, lstm_layers,  bidirectional)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dp = nn.Dropout(0.4)\n",
    "        self.linear1 = nn.Linear(2048,num_classes)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    def forward(self, x):\n",
    "        batch_size,seq_length, c, h, w = x.shape\n",
    "        x = x.view(batch_size * seq_length, c, h, w)\n",
    "        fmap = self.model(x)\n",
    "        x = self.avgpool(fmap)\n",
    "        x = x.view(batch_size,seq_length,2048)\n",
    "        x_lstm,_ = self.lstm(x,None)\n",
    "        return fmap,self.dp(self.linear1(torch.mean(x_lstm,dim = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(2).cuda()\n",
    "a,b = model(torch.from_numpy(np.empty((1,20,3,112,112))).type(torch.cuda.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "def calculate_accuracy(outputs, targets):\n",
    "    batch_size = targets.size(0)\n",
    "\n",
    "    _, pred = outputs.topk(1, 1, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(targets.view(1, -1))\n",
    "    n_correct_elems = correct.float().sum().item()\n",
    "    return 100* n_correct_elems / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, num_epochs, data_loader, model, criterion, optimizer):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    accuracies = AverageMeter()\n",
    "    t = []\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            targets = targets.type(torch.cuda.LongTensor)\n",
    "            inputs = inputs.cuda()\n",
    "        _,outputs = model(inputs)\n",
    "        loss  = criterion(outputs,targets.type(torch.cuda.LongTensor))\n",
    "        acc = calculate_accuracy(outputs, targets.type(torch.cuda.LongTensor))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        accuracies.update(acc, inputs.size(0))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sys.stdout.write(\n",
    "                \"\\r[Epoch %d/%d] [Batch %d / %d] [Loss: %f, Acc: %.2f%%]\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    num_epochs,\n",
    "                    i,\n",
    "                    len(data_loader),\n",
    "                    losses.avg,\n",
    "                    accuracies.avg))\n",
    "    \n",
    "    filename = f'C:/Users/alapa/Documents/Deepfake_detection_using_deep_learning-master/Deepfake_detection_using_deep_learning-master/abhijit/model_checkpoints/new_small_models/train_model_{accuracies.avg:.2f}_epoch_{epoch}.pt'\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    return losses.avg,accuracies.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch,model, data_loader ,criterion):\n",
    "    print('Testing')\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    accuracies = AverageMeter()\n",
    "    pred = []\n",
    "    true = []\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in enumerate(data_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                targets = targets.cuda().type(torch.cuda.FloatTensor)\n",
    "                inputs = inputs.cuda()\n",
    "            _,outputs = model(inputs)\n",
    "            loss = torch.mean(criterion(outputs, targets.type(torch.cuda.LongTensor)))\n",
    "            acc = calculate_accuracy(outputs,targets.type(torch.cuda.LongTensor))\n",
    "            _,p = torch.max(outputs,1) \n",
    "            true += (targets.type(torch.cuda.LongTensor)).detach().cpu().numpy().reshape(len(targets)).tolist()\n",
    "            pred += p.detach().cpu().numpy().reshape(len(p)).tolist()\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            accuracies.update(acc, inputs.size(0))\n",
    "            sys.stdout.write(\n",
    "                    \"\\r[Batch %d / %d]  [Loss: %f, Acc: %.2f%%]\"\n",
    "                    % (\n",
    "                        i,\n",
    "                        len(data_loader),\n",
    "                        losses.avg,\n",
    "                        accuracies.avg\n",
    "                        )\n",
    "                    )\n",
    "        print('\\nAccuracy {}'.format(accuracies.avg))\n",
    "    return true,pred,losses.avg,accuracies.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Output confusion matrix\n",
    "def print_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    print('True positive = ', cm[0][0])\n",
    "    print('False positive = ', cm[0][1])\n",
    "    print('False negative = ', cm[1][0])\n",
    "    print('True negative = ', cm[1][1])\n",
    "    print('\\n')\n",
    "\n",
    "    df_cm = pd.DataFrame(cm, range(2), range(2))\n",
    "    sn.set(font_scale=1.4) # for label size\n",
    "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "    plt.ylabel('Actual label', size = 20)\n",
    "    plt.xlabel('Predicted label', size = 20)\n",
    "    plt.xticks(np.arange(2), ['Fake', 'Real'], size = 16)\n",
    "    plt.yticks(np.arange(2), ['Fake', 'Real'], size = 16)\n",
    "    plt.ylim([2, 0])\n",
    "    plt.show()\n",
    "    calculated_acc = (cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+ cm[1][1])\n",
    "    print(\"Calculated Accuracy\",calculated_acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_loss_avg,test_loss_avg,num_epochs):\n",
    "  loss_train = train_loss_avg\n",
    "  loss_val = test_loss_avg\n",
    "  print(num_epochs)\n",
    "  epochs = range(1,num_epochs+1)\n",
    "  plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "  plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
    "  plt.title('Training and Validation loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "def plot_accuracy(train_accuracy,test_accuracy,num_epochs):\n",
    "  loss_train = train_accuracy\n",
    "  loss_val = test_accuracy\n",
    "  epochs = range(1,num_epochs+1)\n",
    "  plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n",
    "  plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n",
    "  plt.title('Training and Validation accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5#0.001\n",
    "#number of epochs \n",
    "num_epochs = 20\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "train_loss_avg =[]\n",
    "train_accuracy = []\n",
    "test_loss_avg = []\n",
    "test_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1,num_epochs+1):\n",
    "    l, acc = train_epoch(epoch,num_epochs,train_loader,model,criterion,optimizer)\n",
    "    train_loss_avg.append(l)\n",
    "    train_accuracy.append(acc)\n",
    "    true,pred,tl,t_acc = test(epoch,model,valid_loader,criterion)\n",
    "    test_loss_avg.append(tl)\n",
    "    test_accuracy.append(t_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_loss_avg,test_loss_avg,len(train_loss_avg))\n",
    "plot_accuracy(train_accuracy,test_accuracy,len(train_accuracy))\n",
    "print(confusion_matrix(true,pred))\n",
    "print_confusion_matrix(true,pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
